{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61476da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "\n",
    "FILEPATH = Path('../')\n",
    "load_dotenv('../secrets/.env', override=True)\n",
    "\n",
    "GCP_PROJECT_NAME = os.getenv('GCP_PROJECT_NAME')\n",
    "GCP_DATASET_NAME = os.getenv('GCP_DATASET_NAME')\n",
    "\n",
    "client = bigquery.Client.from_service_account_json(FILEPATH / 'secrets' / os.getenv('SERVICE_ACCOUNT_FILE'))\n",
    "dataset = client.dataset(GCP_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c0bb6",
   "metadata": {},
   "source": [
    "# Loading Data into Bigquery Tables\n",
    "\n",
    "One time loading process to insert data into bigquery tables. Automatic pipelines will be placed in another script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bd067",
   "metadata": {},
   "source": [
    "## Investment\n",
    "\n",
    "### ASNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "546f7e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILEPATH / 'data/investment_vehicles/asnb.json', 'r') as f:\n",
    "    asnb_data = json.loads(f.read())\n",
    "    asnb_df = pd.DataFrame.from_records(asnb_data['data'])\n",
    "    asnb_df['year'] = pd.to_datetime(asnb_df['year'], format='%Y')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "asnb_df.to_csv(buf, index=False)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2647f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table investment_asnb...\n",
      "Job finished\n",
      "Successfully loaded 36 rows into the table\n"
     ]
    }
   ],
   "source": [
    "table_ref = dataset.table('investment_asnb')\n",
    "\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae8bdc",
   "metadata": {},
   "source": [
    "### KWSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c4733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILEPATH / 'data/investment_vehicles/kwsp.json', 'r') as f:\n",
    "    kwsp_data = json.loads(f.read())\n",
    "    kwsp_df = pd.DataFrame.from_records(kwsp_data['data'])\n",
    "    kwsp_df['year'] = pd.to_datetime(kwsp_df['year'], format='%Y')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "kwsp_df.to_csv(buf, index=False)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbf2b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table investment_kwsp...\n",
      "Job finished\n",
      "Successfully loaded 74 rows into the table\n"
     ]
    }
   ],
   "source": [
    "table_ref = dataset.table('investment_kwsp')\n",
    "\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce39bed",
   "metadata": {},
   "source": [
    "### Tabung Haji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5ddf8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILEPATH / 'data/investment_vehicles/tabung_haji.json', 'r') as f:\n",
    "    tabung_haji_data = json.loads(f.read())\n",
    "    tabung_haji_df = pd.DataFrame.from_records(tabung_haji_data['data'])\n",
    "    tabung_haji_df['year'] = pd.to_datetime(tabung_haji_df['year'], format='%Y')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "tabung_haji_df.to_csv(buf, index=False)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82156311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table investment_tabung_haji...\n",
      "Job finished\n",
      "Successfully loaded 31 rows into the table\n"
     ]
    }
   ],
   "source": [
    "table_ref = dataset.table('investment_tabung_haji')\n",
    "\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f7b15",
   "metadata": {},
   "source": [
    "## Bonds\n",
    "\n",
    "### Government Investment Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72869ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILEPATH / 'data/investment_vehicles/government_investment_issues.json', 'r') as f:\n",
    "    gov_issue_data = json.loads(f.read())\n",
    "    gov_issue_df = pd.DataFrame.from_records(gov_issue_data['data'])\n",
    "\n",
    "buf = io.BytesIO()\n",
    "gov_issue_df.to_csv(buf, index=False)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "539cdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table bonds_government_investment_issues...\n",
      "Job finished\n",
      "Successfully loaded 11963 rows into the table\n"
     ]
    }
   ],
   "source": [
    "table_ref = dataset.table('bonds_government_investment_issues')\n",
    "\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b255",
   "metadata": {},
   "source": [
    "### Malaysian Government Securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efbe390e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILEPATH / 'data/investment_vehicles/malaysian_government_securities.json', 'r') as f:\n",
    "    gov_securities_data = json.loads(f.read())\n",
    "    gov_securities_df = pd.DataFrame.from_records(gov_securities_data['data'])\n",
    "\n",
    "buf = io.BytesIO()\n",
    "gov_securities_df.to_csv(buf, index=False)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4be047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table bonds_government_securitites...\n",
      "Job finished\n",
      "Successfully loaded 19147 rows into the table\n"
     ]
    }
   ],
   "source": [
    "table_ref = dataset.table('bonds_government_securitites')\n",
    "\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba7cd1",
   "metadata": {},
   "source": [
    "## Pricecatcher\n",
    "\n",
    "### Item Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d7ebfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table pricecatcher_item_lookup...\n",
      "Job finished\n",
      "Successfully loaded 757 rows into the table\n"
     ]
    }
   ],
   "source": [
    "with open(FILEPATH / 'data/consumer_metrics/lookup_item.parquet', 'rb') as f:\n",
    "    buf = io.BytesIO(f.read())\n",
    "    buf.seek(0)\n",
    "\n",
    "table_ref = dataset.table('pricecatcher_item_lookup')\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0f745",
   "metadata": {},
   "source": [
    "### Premise Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8ba8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table pricecatcher_premise_lookup...\n",
      "Job finished\n",
      "Successfully loaded 2995 rows into the table\n"
     ]
    }
   ],
   "source": [
    "premise_df = (\n",
    "    pd.read_parquet(FILEPATH / 'data/consumer_metrics/lookup_premise.parquet')\n",
    "    .dropna(how='all', subset='premise_code')\n",
    "    .astype({'premise_code': int})\n",
    ")\n",
    "\n",
    "buf = io.BytesIO()\n",
    "premise_df.to_parquet(buf, index=False)\n",
    "buf.seek(0)\n",
    "\n",
    "table_ref = dataset.table('pricecatcher_premise_lookup')\n",
    "initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "load_job = client.load_table_from_file(\n",
    "    buf, table_ref, job_config=job_config\n",
    ")\n",
    "print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "load_job.result()\n",
    "loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "print('Job finished')\n",
    "print(f'Successfully loaded {loaded_rows} rows into the table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4c0f6",
   "metadata": {},
   "source": [
    "### Pricecatcher Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a4eef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_slice(df, batch_size):\n",
    "    n = df.shape[0]\n",
    "    for i in range(0, n//batch_size + 1):\n",
    "        yield df.loc[batch_size*i:batch_size*(i+1)-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e02e775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date            datetime64[s]\n",
       "premise_code            int32\n",
       "item_code               int32\n",
       "price                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_df = pd.read_parquet(FILEPATH / 'data/consumer_metrics/pricecatcher_annual_2022.parquet')\n",
    "transaction_df['date'] = pd.to_datetime(transaction_df['date'], format='%Y-%m-%d')\n",
    "transaction_df = transaction_df.reset_index(drop=True).astype({'date': 'datetime64[s]'})\n",
    "\n",
    "transaction_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7fb60d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 1000000 rows into the table\n",
      "Loading data into table pricecatcher_transactional_record...\n",
      "Job finished\n",
      "Successfully loaded 403983 rows into the table\n"
     ]
    }
   ],
   "source": [
    "for idx, df_slice in enumerate(batch_slice(transaction_df, 1_000_000)):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    df_slice.to_parquet(buf, index=False)\n",
    "    buf.seek(0)\n",
    "\n",
    "    table_ref = dataset.table('pricecatcher_transactional_record')\n",
    "    initial_row_count = client.get_table(table_ref).num_rows\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "    # job_config.skip_leading_rows = 1\n",
    "    job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "    load_job = client.load_table_from_file(\n",
    "        buf, table_ref, job_config=job_config\n",
    "    )\n",
    "    print(f'Loading data into table {table_ref.table_id}...')\n",
    "\n",
    "    load_job.result()\n",
    "    loaded_rows = client.get_table(table_ref).num_rows - initial_row_count\n",
    "\n",
    "    print('Job finished')\n",
    "    print(f'Successfully loaded {loaded_rows} rows into the table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
